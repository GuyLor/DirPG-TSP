{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder():\n",
    "    def __call__(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          inputs: [batch_size, num_nodes, node_dim] tensor\n",
    "        Returns:\n",
    "          [batch_size, num_nodes, embedding_dim] tensor\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "class Decoder():\n",
    "    def __call__(self, encoder_output, context_node_id, visited_mask):\n",
    "        \"\"\"Looks up embedding for context node, then does masked attention over encoder outputs.\n",
    "        \n",
    "        Result is log probs over next nodes.\n",
    "        \n",
    "        Args:\n",
    "          inputs: [batch_size, num_nodes, embedding_dim]\n",
    "          ...\n",
    "          \n",
    "        Returns:\n",
    "          [batch_size, num_nodes] tensor of log probs with masking applied.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class BatchedState():\n",
    "    def __init__():\n",
    "        \"\"\"\n",
    "        Should mirror the StateTSP of \"attention learn how to solve routing problems\"\n",
    "        \"\"\"\n",
    "\n",
    "class BatchedGraphs():\n",
    "    def __init__(self, edge_costs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          edge_costs: [batch_size, num_edges, 2]\n",
    "        \"\"\"\n",
    "        self._edge_costs = edge_costs\n",
    "        \n",
    "        \n",
    "class BatchedHeaps():\n",
    "    def __init__(self, batched_graphs):\n",
    "        pass\n",
    "    \n",
    "    def pop_batch(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_random_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a26e4994a23f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mraw_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_random_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# C++\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mgraphs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchedGraphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_costs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (TODO: convert from distance matrix to edges)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_random_data' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: generate random edge costs\n",
    "num_nodes_per_graph = 20\n",
    "num_batches = 100\n",
    "batch_size = 10\n",
    "\n",
    "raw_graph = generate_random_data()\n",
    "# C++\n",
    "graphs = BatchedGraphs(edge_costs) # (TODO: convert from distance matrix to edges)\n",
    "graphs.pre_sort_edges()\n",
    "\n",
    "# Python / GPU\n",
    "encoder = Encoder()\n",
    "decoder = Decoder()   \n",
    "encoder_output = Encoder(graphs)\n",
    "\n",
    "# Existing state class of Attention project\n",
    "state = StateTsp.init_state(graphs)\n",
    "\n",
    "# C++ <-- python\n",
    "cpp_node_heap <-- python_obj_node_heap # batch of tensors (several attributes)\n",
    "cpp_graphs <-- graphs # batch of graphs \n",
    "# C++\n",
    "queues = BatchedHeaps(cpp_graphs, cpp_state) # TODO: iterate over graph_size and create C++ Heap for each\n",
    "\n",
    "# Python\n",
    "while len(queues) > 0: # while queues not empty\n",
    "    # C++\n",
    "    current_nodes = queues.pop_batch() # inside nested for-loop over batch of c++ heap class\n",
    "    \n",
    "    # C++ -> Python -- small amount of memory\n",
    "    # current_nodes:\n",
    "    #  * prefixes / visited masks\n",
    "    #  * context nodes (TODO: what does that mean?)\n",
    "    \n",
    "    # CPU -> GPU\n",
    "    # * current_nodes\n",
    "    # NOT node_hiddens (should already be on GPU, so no cost to send in inner loop)\n",
    "    \n",
    "    # Python / GPU\n",
    "    action_log_probs = decoder(encoder_output, state)  # [batch, num_nodes] child log probs\n",
    "    \n",
    "    # ?? (probably python)\n",
    "    special_actions = sample(action_log_probs)\n",
    "\n",
    "    # GPU -> CPU\n",
    "    # * special children [batch_size] of ints\n",
    "\n",
    "    # ok to do on python CPU\n",
    "    other_actions = get_other_actions(current_nodes, special_actions)\n",
    "    \n",
    "    # Convert actions to nodes\n",
    "    # python or C++?\n",
    "    \n",
    "    # Python -> C++ -- no huge tensors\n",
    "    # * batch of prefixes (though maybe this can live in C++ already)\n",
    "    # * special children\n",
    "    # * other children\n",
    "    \n",
    "    # C++\n",
    "    # get batch of truncated gumbels\n",
    "    # [batch] tensors of costs\n",
    "    special_spanning_tree_costs = graphs.spanning_tree_cost(current_nodes.prefixes, special_children)\n",
    "    # [batch] tensors of costs\n",
    "    other_spanning_tree_costs = graphs.spanning_tree_cost(current_nodes.prefixes, other_children)\n",
    "    \n",
    "    # C++\n",
    "    # missing: first node, next possible actions\n",
    "    queues.batch_push(special_children, special_spanning_tree_costs, special_gumbels)\n",
    "    queues.batch_push(other_children, other_spanning_tree_costs, other_gumbels)\n",
    "   \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STARTS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import kruskals_cpp\n",
    "import dirpg_cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_model\n",
    "from dirpg_tsp import dirpg as dirpg_m\n",
    "from utils import utils_gumbel\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's import the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_distance_matrix(n):\n",
    "    loc = torch.FloatTensor(n, 2).uniform_(0, 1)\n",
    "    return (loc[:, None, :] - loc[None, :, :]).norm(p=2, dim=-1)\n",
    "\n",
    "def generate_data(size,num_samples):\n",
    "    return torch.FloatTensor(num_samples,size, 2).uniform_(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class search_params:\n",
    "    def __init__(self,\n",
    "                 max_interactions=200,\n",
    "                 alpha=1.0,\n",
    "                 epsilon=2.0,\n",
    "                 heuristic='mst',\n",
    "                 independent_gumbel=False,\n",
    "                 first_improvement=False,\n",
    "                 dynamic_weighting = False,\n",
    "                 dfs_like=False,\n",
    "                 not_prune=False):        \n",
    "        self.max_interactions = max_interactions\n",
    "        self.first_improvement = first_improvement\n",
    "        self.dynamic_weighting = dynamic_weighting\n",
    "        self.independent_gumbel = independent_gumbel\n",
    "        self.heuristic = heuristic\n",
    "        self.dfs_like = dfs_like\n",
    "        self.prune = not not_prune\n",
    "        self.alpha=alpha\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "\n",
    "class CPP_manager:\n",
    "    def __init__(self, root_node, batch_size, graph_size):\n",
    "        \n",
    "\n",
    "        root_cpp_converter = dirpg_cpp.convert_python_to_cpp(*root_node.get())\n",
    "        \n",
    "        print(\"cpp_converter PYTHON\")\n",
    "        print(root_cpp_converter)\n",
    "        \n",
    "        self.batched_heaps = dirpg_cpp.BatchedHeaps(root_cpp_converter, batch_size, graph_size) \n",
    "        #self.objects_dict = {root_cpp_converter:batch_size}\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def pop_batch(self):\n",
    "        \n",
    "        (parents,to_remove), trajs = self.batched_heaps.pop_batch()\n",
    "        \"\"\"\n",
    "        for converter in to_remove:\n",
    "            self.objects_dict[converter] -= 1\n",
    "            if self.objects_dict[converter] == 0:\n",
    "                del self.objects_dict[converter]\n",
    "        \"\"\"\n",
    "        return Node.create_node_from_cpp(parents), trajs\n",
    "        \n",
    "        \n",
    "    def push_batch(self,node, to_avoid):\n",
    "        cpp_converter = dirpg_cpp.convert_python_to_cpp(*node.get())\n",
    "        #self.objects_dict.update({cpp_converter:copy.copy(self.batch_size)})\n",
    "        self.batched_heaps.push_batch(cpp_converter, to_avoid)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trajectory:\n",
    "    main_queue = None\n",
    "    def __init__(self):\n",
    "\n",
    "        self.num_candidates = 0\n",
    "        self.t_opt = None\n",
    "        self.t_direct = None\n",
    "    \n",
    "        \n",
    "    def set_trajectory(self, cpp_traj):\n",
    "        self.num_candidates += 1\n",
    "        if self.num_candidates == 1:  # if t_opt            \n",
    "            self.t_opt = cpp_traj\n",
    "            self.t_direct = cpp_traj\n",
    "            self.main_queue.lower_bound[cpp_traj.idx] = cpp_traj.objective\n",
    "\n",
    "        else:\n",
    "            if cpp_traj.objective > self.t_direct.objective:\n",
    "\n",
    "                self.t_direct = cpp_traj\n",
    "                self.main_queue.lower_bound[cpp_traj.idx] = cpp_traj.objective\n",
    "\n",
    "    def print_trajectory(self):\n",
    "        print('--------  Trajectory  ---------')\n",
    "        print('trajectories_list:  ', self.trajectories_list)\n",
    "        print('t_opt:  ', self.t_opt)\n",
    "        print('t_direct:  ', self.t_direct)\n",
    "        print('objective:  ', self.objective)\n",
    "        print('-------------------------------')\n",
    "        \n",
    "class Node:\n",
    "    epsilon = 1.0\n",
    "    alpha = 2.0  # 2: full walk on the MST\n",
    "    dynamic_weighting = True\n",
    "    graph_size = 20\n",
    "    heuristic = ''\n",
    "\n",
    "    mst_edges = None\n",
    "    mst_val = None\n",
    "\n",
    "    def __init__(self,\n",
    "                 ids,  # ids of sample in the batch - maybe to remove\n",
    "                 t,  # time-step (not necessarily the same t for each sample)\n",
    "                 next_actions,  # mask: 1 available, 0 unavailable\n",
    "                 not_visited,   # mask: 1 available, 0 unavailable (note that next_actions!=not_visited)\n",
    "                 prefix,  # e.g.: [2,6,1,0,-1,-1,-1,-1]\n",
    "                 lengths,\n",
    "                 cur_coord=None,\n",
    "                 done=False,\n",
    "                 logprob_so_far=0,\n",
    "                 bound_togo=None,\n",
    "                 max_gumbel=None,\n",
    "                 is_t_opt=True):  #  true: opt, false: direct\n",
    "\n",
    "\n",
    "        self.ids = ids\n",
    "        self.t = t\n",
    "        self.next_actions = next_actions        \n",
    "        self.not_visited = not_visited        \n",
    "        self.prefix = prefix\n",
    "        \n",
    "\n",
    "        self.lengths = lengths\n",
    "        self.cur_coord = cur_coord\n",
    "        self.done = done\n",
    "        self.logprob_so_far = logprob_so_far\n",
    "        self.max_gumbel = max_gumbel\n",
    "        self.is_t_opt = is_t_opt  \n",
    "        self.cpp_conv = None\n",
    "        \n",
    "        if self.dynamic_weighting:\n",
    "            self.alpha = 1 + self.alpha*(1-self.t/self.graph_size)\n",
    "\n",
    "        \n",
    "\n",
    "        self.bound_togo = bound_togo\n",
    "\n",
    "        self.eps_reward = self.epsilon * (self.lengths + self.alpha * self.bound_togo)\n",
    "\n",
    "        self.priority = self.get_priority()\n",
    "        self.objective = self.priority\n",
    "        self.upper_bound = self.priority  # self.get_upper_bound()\n",
    "\n",
    "    def get_priority(self, alpha=2):\n",
    "        return self.max_gumbel + self.eps_reward\n",
    "\n",
    "    def get_priority_max_gumbel(self):\n",
    "        return self.max_gumbel\n",
    "\n",
    "    def get_upper_bound(self, city=None):\n",
    "        return self.max_gumbel + self.epsilon * (self.lengths + 1.0*self.bound_togo) #self.lengths + self.bound_length_togo()\n",
    "\n",
    "\n",
    "    def get_objective(self):\n",
    "        \"\"\"Computes the objective of the trajectory.\n",
    "        Only used if a node is terminal.\n",
    "        \"\"\"\n",
    "        return self.max_gumbel + self.epsilon * self.lengths\n",
    "    \n",
    "    def pack(self):\n",
    "        \"\"\"\n",
    "        pack node to cpp API: float, bool, obj_adrs, int (row in the original batch)\n",
    "             \n",
    "        \"\"\"         \n",
    "        return [[self.priority[i].item(), self.is_t_opt[i].item(), self, i] for i in range(self.ids.size(0))]\n",
    "    \n",
    "    def get(self):\n",
    "        return self.priority, self.is_t_opt, self.done, self.ids, self.t, self.next_actions, self.not_visited,\\\n",
    "               self.prefix, self.lengths, self.cur_coord, self.logprob_so_far, self.bound_togo, self.max_gumbel,\\\n",
    "               self\n",
    "    \n",
    "    def print(self):\n",
    "        print(' -----------    Node     -----------')\n",
    "        print('ids:  ', self.ids)\n",
    "        #print('first_a:  ', self.first_a)\n",
    "        print('prefix:  ', self.prefix)\n",
    "        print('not visited:  ', self.not_visited)\n",
    "        print('next_actions:  ', self.next_actions)\n",
    "        print('t:  ', self.t)\n",
    "        #print('distance matrix:')\n",
    "        print('max_gumbel:  ', self.max_gumbel)\n",
    "        print('epsilon:   ',self.epsilon)\n",
    "        print('alpha:   ', self.alpha)\n",
    "        print('alpha * len_togo:   ', self.alpha * self.bound_togo)\n",
    "        print('eps_reward: ', self.eps_reward, self.eps_reward.size())\n",
    "        print('priority: ', self.priority)\n",
    "        print('objective: ', self.objective)\n",
    "        #print('upper bound: ',self.upper_bound)\n",
    "        print('lengths:  ', self.lengths)\n",
    "        print('bound length togo: ', self.bound_togo)\n",
    "        print('done:  ', self.done)\n",
    "        print('logprob_so_far:  ', self.logprob_so_far)\n",
    "\n",
    "        print('is_t_opt:  ', self.is_t_opt)\n",
    "        print(' -------------------------------')\n",
    "        \n",
    "    @staticmethod\n",
    "    def create_node_from_cpp(args):\n",
    "        return Node(ids=args[0],\n",
    "                    t=args[1],\n",
    "                    next_actions=args[2].bool(),\n",
    "                    not_visited=args[3].bool(),\n",
    "                    prefix=args[4],\n",
    "                    lengths=args[5],\n",
    "                    cur_coord=args[6],\n",
    "                    done=args[7].bool(),\n",
    "                    logprob_so_far=args[8],\n",
    "                    bound_togo=args[9],\n",
    "                    max_gumbel=args[10],\n",
    "                    is_t_opt=args[11].bool()) \n",
    "    @staticmethod\n",
    "    def create_node_from_cpp_list(nodes_list):\n",
    "        atts = [[] for _ in range(12)]\n",
    "        for idx, node in enumerate(nodes_list):\n",
    "            atts[0].append(node.python_node_object.ids[node.idx_original])\n",
    "            atts[1].append(node.python_node_object.t[node.idx_original])\n",
    "            atts[2].append(node.python_node_object.next_actions[node.idx_original])\n",
    "            atts[3].append(node.python_node_object.not_visited[node.idx_original])\n",
    "            atts[4].append(node.python_node_object.prefix[node.idx_original])\n",
    "            atts[5].append(node.python_node_object.lengths[node.idx_original])\n",
    "            atts[6].append(node.python_node_object.cur_coord[node.idx_original])\n",
    "            atts[7].append(node.python_node_object.done[node.idx_original].bool())\n",
    "            atts[8].append(node.python_node_object.logprob_so_far[node.idx_original])\n",
    "            atts[9].append(node.python_node_object.bound_togo[node.idx_original])\n",
    "            atts[10].append(node.python_node_object.max_gumbel[node.idx_original])\n",
    "            atts[11].append(node.python_node_object.is_t_opt[node.idx_original])\n",
    "        \n",
    "        n = Node( ids=torch.stack(atts[0]),  \n",
    "                  t=torch.stack(atts[1]), \n",
    "                  next_actions=torch.stack(atts[2]),\n",
    "                  not_visited=torch.stack(atts[3]),\n",
    "                  prefix=torch.stack(atts[4]),\n",
    "                  lengths=torch.stack(atts[5]),\n",
    "                  cur_coord=torch.stack(atts[6]),\n",
    "                  done=torch.stack(atts[7]),\n",
    "                  logprob_so_far=torch.stack(atts[8]),\n",
    "                  bound_togo=torch.stack(atts[9]),\n",
    "                  max_gumbel=torch.stack(atts[10]),\n",
    "                  is_t_opt=torch.stack(atts[11]))\n",
    "  \n",
    "        return n\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriorityQueue:\n",
    "    def __init__(self,\n",
    "                 init_state,\n",
    "                 graphs,\n",
    "                 epsilon,\n",
    "                 search_params,\n",
    "                 inference=False\n",
    "                 ):\n",
    "\n",
    "        special_action = init_state.prev_a\n",
    "        not_visited = ~init_state.visited.bool()\n",
    "        \n",
    "        dummy_idx = special_action.repeat(1,init_state.loc.size(-1)).unsqueeze(1)        \n",
    "        self.first_coord = torch.gather(init_state.loc, 1, dummy_idx)\n",
    "        \n",
    "        self.graph_size = graphs.size(1)\n",
    "        self.batch_size = graphs.size(0)\n",
    "        ##########################################\n",
    "        #           global nodes parameters      #\n",
    "        Node.alpha = search_params.alpha\n",
    "        Node.epsilon = epsilon\n",
    "        Node.dynamic_weighting = search_params.dynamic_weighting\n",
    "        Node.heuristic = search_params.heuristic\n",
    "        Node.graph_size = self.graph_size\n",
    "        ##########################################\n",
    "        #           TODO: sort edges, kruskals_cpp\n",
    "        \n",
    "        #self.mst_edges = mst.prim_pytorch(distance_mat)\n",
    "        #self.mst_val = self.mst_edges.sum()\n",
    "        \n",
    "        device = not_visited.device\n",
    "        prefix=-torch.ones(self.batch_size,self.graph_size, device=device, dtype=torch.long)\n",
    "        #prefix = p.scatter_(-1,torch.zeros(self.batch_size, dtype=torch.long), special_action) #\n",
    "        prefix[:,0] = special_action.squeeze(-1)\n",
    "        root_node = Node(ids=init_state.ids,\n",
    "                         t=init_state.i,\n",
    "                         next_actions=not_visited,  # torch.tensor(not_visited),  # number of cities\n",
    "                         not_visited=not_visited,\n",
    "                         prefix=prefix,\n",
    "                         lengths=torch.zeros(self.batch_size,1,device=device),\n",
    "                         cur_coord=self.first_coord,\n",
    "                         done=torch.zeros(self.batch_size,1,device=device, dtype=torch.bool),\n",
    "                         logprob_so_far=torch.zeros(self.batch_size,1,device=device),\n",
    "                         bound_togo=-4.0*torch.ones(self.batch_size,1,device=device), #self.mst_val,\n",
    "                         max_gumbel=utils_gumbel.sample_gumbel(self.batch_size).unsqueeze(-1),\n",
    "                         is_t_opt=torch.ones(self.batch_size,device=device, dtype=torch.bool))\n",
    "      \n",
    "\n",
    "        \n",
    "        self.cpp_heaps = CPP_manager(root_node, self.batch_size, self.graph_size)\n",
    "        Trajectory.main_queue = self\n",
    "        self.batch_trajectories = [Trajectory() for _ in range(self.batch_size)]\n",
    "        self.lower_bound = [-float('Inf') for _ in range(self.batch_size)]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.t_opt = None\n",
    "        self.t_direct = None\n",
    "\n",
    "\n",
    "        self.num_interactions = 0\n",
    "        self.first_improvement = search_params.first_improvement\n",
    "        self.max_interactions = search_params.max_interactions\n",
    "        \n",
    "        self.dynamic_weighting = search_params.dynamic_weighting\n",
    "        self.inference = inference\n",
    "        \n",
    "        self.p = torch.ones(self.batch_size, dtype=torch.bool)*search_params.prune\n",
    "        self.prune = torch.zeros(self.p.size(), dtype=torch.bool)\n",
    "\n",
    "    def pop(self):\n",
    "        \n",
    "        current_node, trajs_list = self.cpp_heaps.pop_batch()\n",
    "        #print('parents_list: ', len(parents_list), 'trajs_list: ', len(trajs_list))\n",
    "        for t in trajs_list:\n",
    "            self.batch_trajectories[t.idx].set_trajectory(t)\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        if self.num_interactions >= self.max_interactions:\n",
    "            return 'break'\n",
    "\n",
    "        if self.prune and self.lower_bound > parent.upper_bound:\n",
    "            self.prune_count += 1\n",
    "            return self.pop()\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        return current_node\n",
    "\n",
    "\n",
    "    def expand(self, state, current_node, logprobs):\n",
    "        print(current_node.is_t_opt)\n",
    "        self.num_interactions += 1\n",
    "        special_action = state.prev_a\n",
    "        #visited = state.visited_.bool()\n",
    "        not_visited = current_node.not_visited.scatter(-1, special_action.unsqueeze(-1), False) \n",
    "        is_done = torch.all(~not_visited.transpose(1,2), dim=1, keepdim=False)\n",
    "        prefix = current_node.prefix.scatter(-1, state.i-1, special_action)        \n",
    "        cur_coord = state.loc[current_node.ids, special_action]\n",
    "        \n",
    "        lengths = -state.lengths #.squeeze(-1)       \n",
    "\n",
    "        lengths = torch.where(is_done,\n",
    "                              lengths - (self.first_coord - cur_coord).norm(p=2, dim=-1),\n",
    "                              lengths) # add distance to first node if it's complete trajectory\n",
    "       \n",
    "        \n",
    "\n",
    "        lp_special = torch.gather(logprobs, -1, special_action)\n",
    "\n",
    "        special_child = Node(\n",
    "            ids=state.ids,\n",
    "            t=current_node.t + 1,\n",
    "            not_visited=not_visited, \n",
    "            prefix=prefix, \n",
    "            lengths=lengths, \n",
    "            cur_coord=cur_coord,\n",
    "            done=is_done,\n",
    "            logprob_so_far=current_node.logprob_so_far + lp_special,\n",
    "            max_gumbel=current_node.max_gumbel,\n",
    "            next_actions=not_visited,\n",
    "            bound_togo=current_node.bound_togo,\n",
    "            is_t_opt=current_node.is_t_opt)\n",
    "\n",
    "        #to_prune = self.prune | (special_child.upper_bound < self.lower_bound)\n",
    "        to_prune = self.prune\n",
    "        \n",
    "        #self.python_nodes_in_queue.update({special_child:self.batch_size})\n",
    "        self.cpp_heaps.push_batch(special_child, to_prune.squeeze(0))\n",
    "        \"\"\"\n",
    "        if self.prune and special_child.upper_bound < self.lower_bound:\n",
    "            self.prune_count += 1\n",
    "        else:\n",
    "            heapq.heappush(self.queue, special_child)\n",
    "\n",
    "        \"\"\"\n",
    "        # Sample the max gumbel for the non-chosen actions and create an \"other\n",
    "        # children\" node if there are any alternatives left.\n",
    "\n",
    "        other_actions = current_node.next_actions.scatter(-1, special_action.unsqueeze(-1), False)\n",
    "\n",
    "        assert torch.all(other_actions.sum(-1) == current_node.next_actions.sum(-1) - 1),'other_actions.sum(-1): {} current_node.next_actions.sum(-1) - 1): {}'.format(other_actions.sum(-1),current_node.next_actions.sum(-1) - 1)\n",
    "\n",
    "        if not self.inference:\n",
    "            #num_entries_to_compute_per_sample = other_actions.sum(-1).view(-1)\n",
    "          \n",
    "            exp = torch.exp(logprobs)\n",
    "\n",
    "            exp[~other_actions.squeeze(1)] = 0\n",
    "            other_max_location = torch.log(exp.sum(-1)).unsqueeze(-1)\n",
    "\n",
    "            other_max_gumbel = utils_gumbel.sample_truncated_gumbel(current_node.logprob_so_far + other_max_location,\n",
    "                                                                    current_node.max_gumbel)\n",
    "            \n",
    "            \n",
    "            ignore = (other_max_location == -np.inf).squeeze(-1)\n",
    "            \n",
    "\n",
    "            other_children = Node(\n",
    "                ids=current_node.ids,\n",
    "                t=current_node.t,\n",
    "                not_visited=current_node.not_visited,\n",
    "                prefix=current_node.prefix,\n",
    "                lengths=current_node.lengths,\n",
    "                cur_coord=current_node.cur_coord,\n",
    "                done=current_node.done,\n",
    "                logprob_so_far=current_node.logprob_so_far,\n",
    "                max_gumbel=other_max_gumbel,\n",
    "                next_actions=other_actions,\n",
    "                bound_togo=current_node.bound_togo, # + self.mst_edges[special_action].sum(),\n",
    "                is_t_opt=current_node.is_t_opt*False)\n",
    "\n",
    "            \"\"\"\n",
    "            if self.prune and other_children.upper_bound < self.lower_bound:\n",
    "                self.prune_count += 1\n",
    "            else:\n",
    "                heapq.heappush(self.queue, other_children)\n",
    "            \"\"\" \n",
    "            \n",
    "            #to_prune = self.prune | (other_children.upper_bound < self.lower_bound) | ignore\n",
    "            to_prune = (self.prune | ignore)\n",
    "            #cpp_conv = dirpg_cpp.NodeConverter(*other_children.get())\n",
    "            #self.python_nodes_in_queue.update({special_child:self.batch_size})\n",
    "            self.cpp_heaps.push_batch(other_children, to_prune)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(x, dirpg):\n",
    "    embeddings = dirpg.encoder(x, only_encoder=True)\n",
    "    \n",
    "    state = dirpg.encoder.problem.make_state(x)\n",
    "    fixed = dirpg.encoder.precompute(embeddings)\n",
    "    return state, fixed\n",
    "\n",
    "def forward_and_update(batch_state, fixed, first_action=None):\n",
    "    dirpg.decoder.eval()\n",
    "    log_p, _ = dirpg.decoder(fixed, batch_state) #fixed[:batch.ids.size(0)]\n",
    "    log_p = log_p[:, 0, :]\n",
    "    _, selected = utils_gumbel.sample_gumbel_argmax(log_p)\n",
    "    if first_action is not None:\n",
    "        selected = torch.ones_like(selected) * first_action\n",
    "        \n",
    "    state = batch_state.update(selected, update_length=True)  # like env.step\n",
    "    return log_p, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [*] Loading model from outputs/tsp_8/jupyter_example/DirPG_20200506T134440/epoch-0.pt\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "graph_size = 8\n",
    "\n",
    "x = torch.rand(batch_size, graph_size, 2) # (x,y) coordinates \n",
    "\n",
    "params = search_params()\n",
    "\n",
    "model, _ = load_model('outputs/tsp_{}/jupyter_example/DirPG_20200506T134440/'.format(graph_size), epoch = 0) # 'pretrained/tsp_100/')\n",
    "model.eval()  # Put in evaluation mode to not track gradients\n",
    "dirpg = dirpg_m.DirPG(model, params)\n",
    "\n",
    "def init_batched_priority_queue(x):    \n",
    "\n",
    "    state, fixed = encode(x, dirpg)\n",
    "    #print('*******  Before init state:  *******')\n",
    "    #state.print_state()\n",
    "    log_p, state = forward_and_update(state, fixed) #  forward decoder and env.step\n",
    "\n",
    "    print('*******  After init state:  *******')\n",
    "    state.print_state()\n",
    "\n",
    "    print('*****  Fixed tensors - living only in python  ***** ')\n",
    "    fixed.print_size() # living only in python and stays fixed during decoding \n",
    "    print()\n",
    "    print(' ***** States tensors --> C++  ***** ')\n",
    "    state.print_size() # \n",
    "    print()\n",
    "    print(' ***** log_p tensors --> C++  ***** ')\n",
    "    print(log_p.size())\n",
    "\n",
    "\n",
    "\n",
    "    pq = PriorityQueue(init_state=state,\n",
    "                       graphs=x,\n",
    "                       epsilon=1.0,\n",
    "                       search_params=params,\n",
    "                       inference=False)\n",
    "    return pq, state, fixed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pq, state, fixed = init_batched_priority_queue(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 3, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 3, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 6, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 4, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 2, -1, -1, -1, -1, -1, -1, -1]])\n",
      "tensor([True, True, True, True, True, True, True, True, True, True])\n",
      "tensor([[ 7,  1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  4, -1, -1, -1, -1, -1, -1],\n",
      "        [ 3,  4, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  7, -1, -1, -1, -1, -1, -1],\n",
      "        [ 3,  2, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  3, -1, -1, -1, -1, -1, -1],\n",
      "        [ 1,  2, -1, -1, -1, -1, -1, -1],\n",
      "        [ 6,  4, -1, -1, -1, -1, -1, -1],\n",
      "        [ 4,  7, -1, -1, -1, -1, -1, -1],\n",
      "        [ 2,  0, -1, -1, -1, -1, -1, -1]])\n",
      "tensor([True, True, True, True, True, True, True, True, True, True])\n",
      "tensor([[ 7,  1,  5, -1, -1, -1, -1, -1],\n",
      "        [ 0,  4,  5, -1, -1, -1, -1, -1],\n",
      "        [ 3,  4,  7, -1, -1, -1, -1, -1],\n",
      "        [ 0,  7,  5, -1, -1, -1, -1, -1],\n",
      "        [ 3,  2,  6, -1, -1, -1, -1, -1],\n",
      "        [ 0,  3,  1, -1, -1, -1, -1, -1],\n",
      "        [ 1,  2,  5, -1, -1, -1, -1, -1],\n",
      "        [ 6,  4,  5, -1, -1, -1, -1, -1],\n",
      "        [ 4,  7,  5, -1, -1, -1, -1, -1],\n",
      "        [ 2,  0,  5, -1, -1, -1, -1, -1]])\n",
      "tensor([True, True, True, True, True, True, True, True, True, True])\n",
      "tensor([[ 7,  1,  5,  6, -1, -1, -1, -1],\n",
      "        [ 0,  4,  5,  3, -1, -1, -1, -1],\n",
      "        [ 3,  4,  7,  5, -1, -1, -1, -1],\n",
      "        [ 0,  7,  5,  3, -1, -1, -1, -1],\n",
      "        [ 3,  2,  6,  1, -1, -1, -1, -1],\n",
      "        [ 0,  3,  1,  7, -1, -1, -1, -1],\n",
      "        [ 1,  2,  5,  0, -1, -1, -1, -1],\n",
      "        [ 6,  4,  5,  7, -1, -1, -1, -1],\n",
      "        [ 4,  7,  5,  2, -1, -1, -1, -1],\n",
      "        [ 2,  0,  5,  6, -1, -1, -1, -1]])\n",
      "tensor([True, True, True, True, True, True, True, True, True, True])\n",
      "tensor([[ 7,  1,  5,  6,  2, -1, -1, -1],\n",
      "        [ 0,  4,  5,  3,  1, -1, -1, -1],\n",
      "        [ 3,  4,  7,  5,  1, -1, -1, -1],\n",
      "        [ 0,  7,  5,  3,  4, -1, -1, -1],\n",
      "        [ 3,  2,  6,  1,  4, -1, -1, -1],\n",
      "        [ 0,  3,  1,  7,  5, -1, -1, -1],\n",
      "        [ 1,  2,  5,  0,  3, -1, -1, -1],\n",
      "        [ 6,  4,  5,  7,  1, -1, -1, -1],\n",
      "        [ 4,  7,  5,  2,  0, -1, -1, -1],\n",
      "        [ 2,  0,  5,  6,  4, -1, -1, -1]])\n",
      "tensor([True, True, True, True, True, True, True, True, True, True])\n",
      "tensor([[ 7,  1,  5,  6,  2,  0, -1, -1],\n",
      "        [ 0,  4,  5,  3,  1,  2, -1, -1],\n",
      "        [ 3,  4,  7,  5,  1,  2, -1, -1],\n",
      "        [ 0,  7,  5,  3,  4,  1, -1, -1],\n",
      "        [ 3,  2,  6,  1,  4,  5, -1, -1],\n",
      "        [ 0,  3,  1,  7,  5,  4, -1, -1],\n",
      "        [ 1,  2,  5,  0,  3,  7, -1, -1],\n",
      "        [ 6,  4,  5,  7,  1,  3, -1, -1],\n",
      "        [ 4,  7,  5,  2,  0,  1, -1, -1],\n",
      "        [ 2,  0,  5,  6,  4,  1, -1, -1]])\n",
      "tensor([True, True, True, True, True, True, True, True, True, True])\n",
      "tensor([[ 7,  1,  5,  6,  2,  0,  3, -1],\n",
      "        [ 0,  4,  5,  3,  1,  2,  7, -1],\n",
      "        [ 3,  4,  7,  5,  1,  2,  0, -1],\n",
      "        [ 0,  7,  5,  3,  4,  1,  2, -1],\n",
      "        [ 3,  2,  6,  1,  4,  5,  7, -1],\n",
      "        [ 0,  3,  1,  7,  5,  4,  2, -1],\n",
      "        [ 1,  2,  5,  0,  3,  7,  4, -1],\n",
      "        [ 6,  4,  5,  7,  1,  3,  2, -1],\n",
      "        [ 4,  7,  5,  2,  0,  1,  6, -1],\n",
      "        [ 2,  0,  5,  6,  4,  1,  3, -1]])\n",
      "tensor([True, True, True, True, True, True, True, True, True, True])\n",
      "tensor([[ 7, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  4,  5, -1, -1, -1, -1, -1],\n",
      "        [ 3, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 3,  2, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  3, -1, -1, -1, -1, -1, -1],\n",
      "        [ 1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 6,  4, -1, -1, -1, -1, -1, -1],\n",
      "        [ 4, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 2, -1, -1, -1, -1, -1, -1, -1]])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([[ 7, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 3, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  6, -1, -1, -1, -1, -1, -1],\n",
      "        [ 3,  2,  4, -1, -1, -1, -1, -1],\n",
      "        [ 0,  3, -1, -1, -1, -1, -1, -1],\n",
      "        [ 1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 6,  4, -1, -1, -1, -1, -1, -1],\n",
      "        [ 4,  5, -1, -1, -1, -1, -1, -1],\n",
      "        [ 2,  0,  5, -1, -1, -1, -1, -1]])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([[ 7,  2, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  5, -1, -1, -1, -1, -1, -1],\n",
      "        [ 3,  5, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0,  6,  7, -1, -1, -1, -1, -1],\n",
      "        [ 3, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 0, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 1, -1, -1, -1, -1, -1, -1, -1],\n",
      "        [ 6,  4, -1, -1, -1, -1, -1, -1],\n",
      "        [ 4,  5,  2, -1, -1, -1, -1, -1],\n",
      "        [ 2,  5, -1, -1, -1, -1, -1, -1]])\n",
      "tensor([False, False, False, False, False, False, False, False, False, False])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    while pq.num_interactions < 10: ## interactions budget\n",
    "\n",
    "        parents = pq.pop()\n",
    "        prev_a = torch.gather(parents.prefix,-1,parents.t-1)\n",
    "        #prev_a = torch.where(parents.t == parents.prefix.size(-1), state.first_a, prev_a)\n",
    "\n",
    "\n",
    "        print(parents.prefix)\n",
    "        visited_ = (~parents.next_actions).type(torch.uint8)\n",
    "\n",
    "        state = state._replace(ids=parents.ids,\n",
    "                        prev_a=prev_a,\n",
    "                        visited_=visited_, # torch.uint8\n",
    "                        lengths=-parents.lengths,\n",
    "                        cur_coord=parents.cur_coord,\n",
    "                        i=parents.t)\n",
    "\n",
    "        log_p, state = forward_and_update(state, fixed)\n",
    "\n",
    "        pq.expand(state,parents, log_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_opt = torch.stack([tp.t_opt.actions for tp in pq.batch_trajectories])\n",
    "t_direct = torch.stack([tp.t_direct.actions for tp in pq.batch_trajectories])\n",
    "\n",
    "t_opt_costs = torch.tensor([tp.t_opt.cost for tp in pq.batch_trajectories])\n",
    "t_direct_costs = torch.tensor([tp.t_direct.cost for tp in pq.batch_trajectories])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 7, 6, 1, 0, 2, 5, 3],\n",
      "        [6, 2, 4, 0, 7, 3, 1, 5],\n",
      "        [7, 6, 5, 1, 0, 4, 3, 2],\n",
      "        [4, 3, 7, 6, 0, 2, 5, 1],\n",
      "        [0, 6, 1, 5, 4, 2, 7, 3],\n",
      "        [7, 6, 2, 1, 5, 4, 3, 0],\n",
      "        [1, 3, 7, 2, 6, 4, 0, 5],\n",
      "        [7, 5, 1, 2, 4, 6, 0, 3],\n",
      "        [6, 2, 0, 3, 7, 1, 5, 4],\n",
      "        [3, 1, 0, 5, 2, 7, 6, 4]])\n",
      "tensor([-4.6290, -3.9223, -5.0708, -3.6825, -4.6622, -5.1006, -3.3818, -3.6265,\n",
      "        -3.7672, -3.3746])\n"
     ]
    }
   ],
   "source": [
    "print(t_opt)\n",
    "print(t_opt_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 7, 3, 1, 5, 0, 6, 2],\n",
      "        [6, 2, 0, 7, 1, 4, 5, 3],\n",
      "        [7, 6, 0, 5, 4, 3, 1, 2],\n",
      "        [4, 3, 7, 2, 1, 5, 0, 6],\n",
      "        [0, 6, 1, 4, 5, 3, 7, 2],\n",
      "        [7, 5, 0, 2, 3, 6, 1, 4],\n",
      "        [1, 3, 7, 6, 0, 2, 4, 5],\n",
      "        [7, 5, 1, 2, 4, 3, 0, 6],\n",
      "        [6, 0, 7, 3, 4, 2, 1, 5],\n",
      "        [3, 1, 7, 5, 0, 4, 2, 6]])\n",
      "tensor([-3.6385, -2.7398, -4.2285, -2.9965, -3.8756, -3.2653, -3.0846, -3.3624,\n",
      "        -5.1156, -3.1277])\n"
     ]
    }
   ],
   "source": [
    "print(t_direct)\n",
    "print(t_direct_costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_root(graphs,\n",
    "             epsilon=1.0,\n",
    "             search_params=params):\n",
    "    \n",
    "    state, fixed = encode(x, dirpg)\n",
    "    log_p, init_state = forward_and_update(state, fixed)\n",
    "    \n",
    "    special_action = init_state.prev_a\n",
    "    not_visited = ~init_state.visited.bool()\n",
    "\n",
    "    dummy_idx = special_action.repeat(1,init_state.loc.size(-1)).unsqueeze(1)        \n",
    "    first_coord = torch.gather(init_state.loc, 1, dummy_idx)\n",
    "\n",
    "    graph_size = graphs.size(1)\n",
    "    batch_size = graphs.size(0)\n",
    "    ##########################################\n",
    "    #           global nodes parameters      #\n",
    "    Node.alpha = search_params.alpha\n",
    "    Node.epsilon = epsilon\n",
    "    Node.dynamic_weighting = search_params.dynamic_weighting\n",
    "    Node.heuristic = search_params.heuristic\n",
    "    Node.graph_size = graph_size\n",
    "    ##########################################\n",
    "    #           TODO: sort edges, kruskals_cpp\n",
    "\n",
    "    #self.mst_edges = mst.prim_pytorch(distance_mat)\n",
    "    #self.mst_val = self.mst_edges.sum()\n",
    "\n",
    "    device = not_visited.device\n",
    "    prefix=-torch.ones(batch_size,graph_size, device=device, dtype=torch.long)\n",
    "    #prefix = p.scatter_(-1,torch.zeros(self.batch_size, dtype=torch.long), special_action) #\n",
    "    prefix[:,0] = special_action.squeeze(-1)\n",
    "    root_node = Node(ids=init_state.ids,\n",
    "                     t=init_state.i,\n",
    "                     next_actions=not_visited,  # torch.tensor(not_visited),  # number of cities\n",
    "                     not_visited=not_visited,\n",
    "                     prefix=prefix,\n",
    "                     lengths=torch.zeros(batch_size,1,device=device),\n",
    "                     cur_coord=first_coord,\n",
    "                     done=torch.zeros(batch_size,1,device=device, dtype=torch.bool),\n",
    "                     logprob_so_far=torch.zeros(batch_size,1,device=device),\n",
    "                     bound_togo=-4.0*torch.rand(batch_size,1,device=device), #self.mst_val,\n",
    "                     max_gumbel=utils_gumbel.sample_gumbel(batch_size).unsqueeze(-1),\n",
    "                     is_t_opt=torch.ones(batch_size,device=device, dtype=torch.bool))\n",
    "    return root_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NodeConverter constructor\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "cpp_converter PYTHON\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "BatchedHeaps constructor\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "NodeConverter operator[]\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "HeapNode constructor converter object:\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "Heap constructor\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "NodeConverter operator[]\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "HeapNode constructor converter object:\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "Heap constructor\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "NodeConverter operator[]\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "HeapNode constructor converter object:\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "Heap constructor\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "NodeConverter operator[]\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "HeapNode constructor converter object:\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "Heap constructor\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "NodeConverter operator[]\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "HeapNode constructor converter object:\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "Heap constructor\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "NodeConverter operator[]\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "HeapNode constructor converter object:\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "Heap constructor\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "NodeConverter operator[]\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "HeapNode constructor converter object:\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "Heap constructor\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "NodeConverter operator[]\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "HeapNode constructor converter object:\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "Heap constructor\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "NodeConverter operator[]\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "HeapNode constructor converter object:\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "Heap constructor\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "NodeConverter operator[]\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "HeapNode constructor converter object:\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "Heap constructor\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n"
     ]
    }
   ],
   "source": [
    "root_node = get_root(x)\n",
    "cpp_obj = CPP_manager(root_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#root_node = get_root(x)\n",
    "def somthing(root_node=None):\n",
    "\n",
    "    root_node = get_root(x)\n",
    "    cpp_converter = dirpg_cpp.NodeConverter(*root_node.get())\n",
    "    root_node.cpp_conv = cpp_converter\n",
    "    print('cpp_converter ',cpp_converter)\n",
    "    print('root_node.cpp_conv ', root_node.cpp_conv)\n",
    "    batched_heaps = dirpg_cpp.BatchedHeaps(root_node.cpp_conv)\n",
    "    return batched_heaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "0x11171f5b0 0x11171f5b0\n",
    "0x11171f3b0, 0x11171f3b0\n",
    "0x11175ee70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NodeConverter constructor\n",
      "<dirpg_cpp.NodeConverter object at 0x12368dbb0>\n",
      "NodeConverter operator[]\n",
      "<dirpg_cpp.NodeConverter object at 0x12368dbb0>\n",
      "HeapNode constructor converter object:\n",
      "<dirpg_cpp.NodeConverter object at 0x12368dbb0>\n",
      "NodeConverter operator[]\n",
      "<dirpg_cpp.NodeConverter object at 0x12368dbb0>\n",
      "HeapNode constructor converter object:\n",
      "<dirpg_cpp.NodeConverter object at 0x12368dbb0>\n",
      "NodeConverter operator[]\n",
      "<dirpg_cpp.NodeConverter object at 0x12368dbb0>\n",
      "HeapNode constructor converter object:\n",
      "<dirpg_cpp.NodeConverter object at 0x12368dbb0>\n",
      "NodeConverter operator[]\n",
      "<dirpg_cpp.NodeConverter object at 0x12368dbb0>\n",
      "HeapNode constructor converter object:\n",
      "<dirpg_cpp.NodeConverter object at 0x12368dbb0>\n",
      "NodeConverter operator[]\n",
      "<dirpg_cpp.NodeConverter object at 0x12368dbb0>\n",
      "HeapNode constructor converter object:\n",
      "<dirpg_cpp.NodeConverter object at 0x12368dbb0>\n",
      "NodeConverter operator[]\n",
      "<dirpg_cpp.NodeConverter object at 0x12368dbb0>\n",
      "HeapNode constructor converter object:\n",
      "<dirpg_cpp.NodeConverter object at 0x12368dbb0>\n",
      "NodeConverter operator[]\n",
      "<dirpg_cpp.NodeConverter object at 0x12368dbb0>\n",
      "HeapNode constructor converter object:\n",
      "<dirpg_cpp.NodeConverter object at 0x12368dbb0>\n",
      "NodeConverter operator[]\n",
      "<dirpg_cpp.NodeConverter object at 0x12368dbb0>\n",
      "HeapNode constructor converter object:\n",
      "<dirpg_cpp.NodeConverter object at 0x12368dbb0>\n",
      "NodeConverter operator[]\n",
      "<dirpg_cpp.NodeConverter object at 0x12368dbb0>\n",
      "HeapNode constructor converter object:\n",
      "<dirpg_cpp.NodeConverter object at 0x12368dbb0>\n",
      "NodeConverter operator[]\n",
      "<dirpg_cpp.NodeConverter object at 0x12368dbb0>\n",
      "HeapNode constructor converter object:\n",
      "<dirpg_cpp.NodeConverter object at 0x12368dbb0>\n"
     ]
    }
   ],
   "source": [
    "node = get_root(x)\n",
    "\n",
    "cpp_obj.push_batch(node, torch.zeros(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "Heap::pop\n",
      "after while\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "False\n",
      "after else\n",
      "after push back\n",
      "********************\n",
      "Heap::pop\n",
      "after while\n",
      "<dirpg_cpp.NodeConverter object at 0x12368dbb0>\n",
      "False\n",
      "after else\n",
      "after push back\n",
      "********************\n",
      "Heap::pop\n",
      "after while\n",
      "<dirpg_cpp.NodeConverter object at 0x12368dbb0>\n",
      "False\n",
      "after else\n",
      "after push back\n",
      "********************\n",
      "Heap::pop\n",
      "after while\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d330>\n",
      "False\n",
      "after else\n",
      "after push back\n",
      "********************\n",
      "Heap::pop\n",
      "after while\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "False\n",
      "after else\n",
      "after push back\n",
      "********************\n",
      "Heap::pop\n",
      "after while\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d070>\n",
      "False\n",
      "after else\n",
      "after push back\n",
      "********************\n",
      "Heap::pop\n",
      "after while\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d330>\n",
      "False\n",
      "after else\n",
      "after push back\n",
      "********************\n",
      "Heap::pop\n",
      "after while\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d330>\n",
      "False\n",
      "after else\n",
      "after push back\n",
      "********************\n",
      "Heap::pop\n",
      "after while\n",
      "<dirpg_cpp.NodeConverter object at 0x12368d330>\n",
      "False\n",
      "after else\n",
      "after push back\n",
      "********************\n",
      "Heap::pop\n",
      "after while\n",
      "<dirpg_cpp.NodeConverter object at 0x12368dbb0>\n",
      "False\n",
      "after else\n",
      "after push back\n",
      "after stack_heap_nodes_vec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Node at 0x12329c350>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpp_obj.pop_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#push\n",
    "cpp_converter = dirpg_cpp.NodeConverter(new_python_node)\n",
    "batched_heaps = dirpg_cpp.BatchedHeaps.push_batch(cpp_converter)\n",
    "\n",
    "nodes_ = []\n",
    "for i in range(5):\n",
    "    \n",
    "    nodes_.append(dirpg_cpp.HeapNode(cpp_converter.get_args_for_heap_node_constructor(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555630965592029"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpg_cpp.stack_heap_nodes_vec(nodes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_heaps = dirpg_cpp.BatchedHeaps(cpp_converter)  #root_node.pack()\n",
    "parents = batched_heaps.pop_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_minus_inf_logsumexp(x, idx):\n",
    "    idx = ~idx\n",
    "    x = x.scatter(-1, idx.long(), -float('Inf'))\n",
    "    print(x)\n",
    "    res = torch.logsumexp(x, -1)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_padded_logsumexp(x, idx):\n",
    "    num_entries_to_compute_per_sample = idx.sum(-1).view(-1)\n",
    "    non_even_concat_logprobs = x[idx]\n",
    "    other_max_location = dirpg_cpp.padded_logsumexp(non_even_concat_logprobs,\n",
    "                                                    num_entries_to_compute_per_sample)\n",
    "    \n",
    "    return other_max_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Timer unit: 1e-06 s\n",
    "\n",
    "Total time: 0.00082 s\n",
    "File: <ipython-input-95-8083cf90a2ba>\n",
    "Function: my_padded_logsumexp at line 1\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def my_padded_logsumexp(x, idx):\n",
    "     2         1        247.0    247.0     30.1      num_entries_to_compute_per_sample = idx.sum(-1).view(-1)\n",
    "     3         1        185.0    185.0     22.6      non_even_concat_logprobs = x[idx]\n",
    "     4         1          3.0      3.0      0.4      other_max_location = dirpg_cpp.padded_logsumexp(non_even_concat_logprobs,\n",
    "     5         1        383.0    383.0     46.7                                                      num_entries_to_compute_per_sample)\n",
    "     6                                               \n",
    "     7         1          2.0      2.0      0.2      return other_max_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_logsumexp(x, idx):\n",
    "    exp = torch.exp(x)\n",
    "    exp[~idx] = 0\n",
    "    other_max_location = torch.log(exp.sum(-1))\n",
    "    return other_max_location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Timer unit: 1e-06 s\n",
    "\n",
    "Total time: 0.000516 s\n",
    "File: <ipython-input-101-c0197651927e>\n",
    "Function: naive_logsumexp at line 1\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def naive_logsumexp(x, idx):\n",
    "     2         1        179.0    179.0     34.7      exp = torch.exp(x)\n",
    "     3         1        290.0    290.0     56.2      exp[~idx] = 0\n",
    "     4         1         46.0     46.0      8.9      other_max_location = torch.log(exp.sum(-1))\n",
    "     5         1          1.0      1.0      0.2      return other_max_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "%%%%%%%%%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 5.,  7., 19.,  0., 70.])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(range(5*4)).view(5,4).float()\n",
    "print(x)\n",
    "print('%%%%%%%%%')\n",
    "idx = torch.tensor([[True, True, False,False],\n",
    "                    [True, True, True, False],\n",
    "                    [False, True, True, False],\n",
    "                    [True, True, True, True],\n",
    "                    [False, False, False,False]])\n",
    "\n",
    "x[idx] = 0\n",
    "x.sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6931],\n",
      "        [1.0986],\n",
      "        [0.6931],\n",
      "        [1.3863],\n",
      "        [  -inf]])\n",
      "--------------------\n",
      "tensor([0.6931, 1.0986, 0.6931, 1.3863,   -inf])\n"
     ]
    }
   ],
   "source": [
    "#idx torch.Tensor([[0,2],[3,4]])\n",
    "\n",
    "print(my_padded_logsumexp(x, idx))\n",
    "print('--------------------')\n",
    "print(naive_logsumexp(x, idx))\n",
    "\n",
    "#x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f my_padded_logsumexp my_padded_logsumexp(x, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3133)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.logsumexp(torch.tensor([2.,  3.]),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-05fcaa0e9bdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not callable"
     ]
    }
   ],
   "source": [
    "a = torch.ones(10,requires_grad=True)\n",
    "a.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 1 at dim 1 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-417271979fb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 1 at dim 1 (got 2)"
     ]
    }
   ],
   "source": [
    "torch.gather(x,-1,torch.tensor([[1],[1, 2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 5.],\n",
       "        [1., 5.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[0, 1], [[0, 2], [1, 2]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for dimension 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-7539340c2654>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     [3, 3]])\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m#mask_minus_inf_logsumexp(x, idx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for dimension 0 with size 2"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(range(2*3)).view(2,3).float()\n",
    "print(x)\n",
    "idx = torch.tensor([[0, 2],\n",
    "                    [3, 3]])\n",
    "\n",
    "torch.logsumexp(x[[0, 2], [1, 2]])\n",
    "#mask_minus_inf_logsumexp(x, idx)\n",
    "\n",
    "#x.sum(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  5.,  2.],\n",
       "        [ 3.,  4., 25.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 5.],\n",
       "        [0., 4.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[0,1], [[1, 2], [0,1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting line_profiler\n",
      "  Downloading line_profiler-3.0.2.tar.gz (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 251 kB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: IPython in /Users/guy_l/Research/DirPG_TSP/dirpg/lib/python3.7/site-packages (from line_profiler) (7.13.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/guy_l/Research/DirPG_TSP/dirpg/lib/python3.7/site-packages (from IPython->line_profiler) (46.1.3)\n",
      "Requirement already satisfied: appnope; sys_platform == \"darwin\" in /Users/guy_l/Research/DirPG_TSP/dirpg/lib/python3.7/site-packages (from IPython->line_profiler) (0.1.0)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /Users/guy_l/Research/DirPG_TSP/dirpg/lib/python3.7/site-packages (from IPython->line_profiler) (4.8.0)\n",
      "Requirement already satisfied: pygments in /Users/guy_l/Research/DirPG_TSP/dirpg/lib/python3.7/site-packages (from IPython->line_profiler) (2.6.1)\n",
      "Requirement already satisfied: pickleshare in /Users/guy_l/Research/DirPG_TSP/dirpg/lib/python3.7/site-packages (from IPython->line_profiler) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/guy_l/Research/DirPG_TSP/dirpg/lib/python3.7/site-packages (from IPython->line_profiler) (3.0.5)\n",
      "Requirement already satisfied: backcall in /Users/guy_l/Research/DirPG_TSP/dirpg/lib/python3.7/site-packages (from IPython->line_profiler) (0.1.0)\n",
      "Requirement already satisfied: decorator in /Users/guy_l/Research/DirPG_TSP/dirpg/lib/python3.7/site-packages (from IPython->line_profiler) (4.4.2)\n",
      "Requirement already satisfied: jedi>=0.10 in /Users/guy_l/Research/DirPG_TSP/dirpg/lib/python3.7/site-packages (from IPython->line_profiler) (0.17.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /Users/guy_l/Research/DirPG_TSP/dirpg/lib/python3.7/site-packages (from IPython->line_profiler) (4.3.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/guy_l/Research/DirPG_TSP/dirpg/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->IPython->line_profiler) (0.6.0)\n",
      "Requirement already satisfied: wcwidth in /Users/guy_l/Research/DirPG_TSP/dirpg/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->line_profiler) (0.1.9)\n",
      "Requirement already satisfied: parso>=0.7.0 in /Users/guy_l/Research/DirPG_TSP/dirpg/lib/python3.7/site-packages (from jedi>=0.10->IPython->line_profiler) (0.7.0)\n",
      "Requirement already satisfied: six in /Users/guy_l/Research/DirPG_TSP/dirpg/lib/python3.7/site-packages (from traitlets>=4.2->IPython->line_profiler) (1.14.0)\n",
      "Requirement already satisfied: ipython-genutils in /Users/guy_l/Research/DirPG_TSP/dirpg/lib/python3.7/site-packages (from traitlets>=4.2->IPython->line_profiler) (0.2.0)\n",
      "Building wheels for collected packages: line-profiler\n",
      "  Building wheel for line-profiler (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for line-profiler: filename=line_profiler-3.0.2-cp37-cp37m-macosx_10_13_x86_64.whl size=52831 sha256=e867da2a8968e1e407c9038c3e8b5a2ea9f0b54eb1aefcacb40a8be10193be3b\n",
      "  Stored in directory: /Users/guy_l/Library/Caches/pip/wheels/20/28/5c/69ef18a2dc224b4230d6b88e9c0721ec7d3bb425b71d3248ce\n",
      "Successfully built line-profiler\n",
      "Installing collected packages: line-profiler\n",
      "Successfully installed line-profiler-3.0.2\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/Users/guy_l/Research/DirPG_TSP/dirpg/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse.csgraph as cs\n",
    "import time\n",
    "\n",
    "num_iter = 100\n",
    "batch_size = 10\n",
    "graph_size = 20\n",
    "\n",
    "\n",
    "def run_cs_mst(num_iter, batch_size, graph_size):\n",
    "    graphs = [generate_random_distance_matrix(graph_size) for _ in range(batch_size)]\n",
    "    cost = torch.zeros(num_iter,batch_size)\n",
    "    \n",
    "\n",
    "    for i in range(num_iter):\n",
    "        for j,graph in enumerate(graphs):        \n",
    "            cost[i,j]=np.sum(cs.minimum_spanning_tree(graph))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_distance_matrix_to_batched_edges(num_iter, batch_size, graph_size):\n",
    "    \"\"\"distance_matrix: batch of distance matrices. size: [batch, n, n]\n",
    "    returns weights_and_edges: in shape (batch_size, n * (n - 1) / 2, 3), where\n",
    "    weights_and_edges[.][i] = [weight_i, node1_i, node2_i] for edge i.\"\"\"\n",
    "    \n",
    "    graphs = [generate_random_distance_matrix(graph_size) for _ in range(batch_size)]\n",
    "    graphs = torch.stack(graphs)\n",
    "    \n",
    "    batched_graphs = BatchedGraphs(graphs)\n",
    "    batched_graphs.init_sort()\n",
    "    cost = torch.zeros(num_iter,batch_size)\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        cost[i,:] = batched_graphs.mst_costs()\n",
    "    \n",
    "    \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f run_cs_mst run_cs_mst(num_iter, batch_size, graph_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graphs = [generate_random_distance_matrix(graph_size) for _ in range(batch_size)]\n",
    "torch.stack(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[False, False, False, False, False]],\n",
      "\n",
      "        [[False,  True,  True, False,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  True,  True]]])\n",
      "tensor([[[-0.1026,  0.2537, -0.3111,  0.3591,  1.6911]],\n",
      "\n",
      "        [[-0.3916, -1.4361,  0.5472, -0.3411,  0.7598]],\n",
      "\n",
      "        [[ 0.1138,  0.0881,  1.0145,  1.0903,  0.2113]]])\n",
      "tensor([[0],\n",
      "        [3],\n",
      "        [5]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-1.4361,  0.5472,  0.7598,  0.1138,  0.0881,  1.0145,  1.0903,  0.2113])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import dirpg_cpp\n",
    "from utils import utils_gumbel\n",
    "\n",
    "batch=3\n",
    "graph_size=5\n",
    "my_visited = torch.tensor([[0, 0, 0 ,0, 0],[0,1,1,0,1],[1,1,1,1,1]], dtype=torch.bool).unsqueeze(1)\n",
    "print(my_visited)\n",
    "x = torch.randn(batch,1,graph_size)\n",
    "print(x)\n",
    "#idx = torch.zeros(batch,1,1).scatter_(-1,torch.tensor([[]]))\n",
    "print(my_visited.sum(-1))\n",
    "x[my_visited]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_samples = my_visited.sum(-1).view(-1)\n",
    "concat_logprobs = x[my_visited]\n",
    "other_max_location = dirpg_cpp.padded_logsumexp(concat_logprobs,lengths_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore = other_max_location == -np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True],\n",
       "        [False],\n",
       "        [ True]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[True], [False], [True]]) | ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([])\n",
      "tensor(-inf)\n",
      "3\n",
      "tensor([-0.0211,  0.1169, -1.1231])\n",
      "tensor(0.8872)\n",
      "5\n",
      "tensor([-0.8852,  0.1871,  0.9724, -1.2777, -0.5325])\n",
      "tensor(1.6348)\n"
     ]
    }
   ],
   "source": [
    "other_max_location = dirpg_cpp.padded_logsumexp(concat_logprobs,lengths_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "mu = torch.randn(batch, graph_size)\n",
    "gumbel_max,_= torch.max(utils_gumbel.sample_gumbel(mu),-1,True)\n",
    "print(gumbel_max.size())\n",
    "print(other_max_location.size())\n",
    "other_max_gumbel = utils_gumbel.sample_truncated_gumbel(other_max_location,gumbel_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -inf],\n",
       "        [0.0889],\n",
       "        [1.4026]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_max_gumbel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6125, 0.4573, 0.8552])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  0.6125, 100.4573,   0.8552]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visited = torch.tensor([(True, True, False),\n",
    "                  (True, True, True),\n",
    "                  (False, False, False)])\n",
    "\n",
    "leng = torch.rand(3)\n",
    "print(leng)\n",
    "torch.where(torch.all(visited.t(), dim=0, keepdim=True),leng+100, leng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False,  True]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "~torch.all(visited.t(), 0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited = torch.tensor([[True, True, False],\n",
    "                  [True, True, True],\n",
    "                  [False, False, False]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False,  True, False])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(visited.t(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True, False])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flag = False\n",
    "a = torch.tensor([100, 100, 9])\n",
    "b = torch.tensor([55, 98, 11])\n",
    "print((a > b))\n",
    "torch.all((a > b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10,1,5)\n",
    "\n",
    "torch.any(torch.tensor([-1,2,3], device=x.device) == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include \"ATen/Parallel.h\"\n",
    "#include <torch/extension.h>\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "\n",
    "//#include <pybind11/stl.h>\n",
    "//#include <pybind11/complex.h>\n",
    "//#include <pybind11/functional.h>\n",
    "\n",
    "using namespace std;\n",
    "\n",
    "class BatchedGraphs{\n",
    "    BatchedGraphs(torch::Tensor distance_matrices){\n",
    "\n",
    "    };\n",
    "\n",
    "};\n",
    "torch::Tensor padded_logsumexp(torch::Tensor concat_logprobs, torch::Tensor lengths){\n",
    "    // Args: concat_logprobs: non-even length for each sample in the batch\n",
    "    // length: the slices of logprobs for each sample (i.e. sum(lengths) == concat_logprobs.size())\n",
    "    torch::Tensor logsumexp_res = torch::zeros({lengths.size(0),1});\n",
    "    int prev_len = 0;\n",
    "    for (int i=0; i<lengths.size(0);i++){\n",
    "        int sample_length = lengths[i].item<int>();\n",
    "        torch::Tensor sample_logprobs = torch::zeros({sample_length});\n",
    "        for (int j=0; j < sample_length; j++){\n",
    "            sample_logprobs[j] = concat_logprobs[prev_len+j];\n",
    "        }\n",
    "        logsumexp_res[i] = sample_logprobs.logsumexp({0});\n",
    "        prev_len = prev_len + sample_length;\n",
    "    }\n",
    "    return logsumexp_res;\n",
    "}\n",
    "\n",
    "\n",
    "struct HeapNode {\n",
    "\n",
    "  float priority;\n",
    "  bool t_opt;\n",
    "  bool done;\n",
    "  py::object python_node_object;\n",
    "  int idx_original;\n",
    "\n",
    "\n",
    "\n",
    "  HeapNode(py::list node_args){\n",
    "    priority = node_args[0].cast<float>();\n",
    "    t_opt = node_args[1].cast<bool>();\n",
    "    done = node_args[2].cast<bool>();\n",
    "    python_node_object = node_args[3];\n",
    "    idx_original = node_args[4].cast<int>();\n",
    "\n",
    "    //py::list batched_prefix(python_node_object.attr(\"prefix\"));\n",
    "\n",
    "\n",
    "  }\n",
    "  HeapNode(const HeapNode &other){\n",
    "      priority = other.priority;\n",
    "      t_opt = other.t_opt;\n",
    "      python_node_object = other.python_node_object;\n",
    "      idx_original = other.idx_original;\n",
    "      done = other.done;\n",
    "  }\n",
    "\n",
    "  bool operator<(const HeapNode &other) const {\n",
    "     if (t_opt == other.t_opt){\n",
    "        return priority < other.priority;\n",
    "     }\n",
    "     if (t_opt && !other.t_opt){\n",
    "        return false;\n",
    "     }\n",
    "     else{\n",
    "        return true;\n",
    "     }\n",
    "  }\n",
    "\n",
    "};\n",
    "\n",
    "\n",
    "class NodeConvertor{\n",
    "    static const batch_size = 100;\n",
    "    torch::Tensor ids = torch::zeros({batch_size,1});\n",
    "    torch::Tensor t = torch::zeros({batch_size,1});\n",
    "    torch::Tensor next_actions = torch::zeros({batch_size,1,8});\n",
    "    torch::Tensor not_visited = torch::zeros({batch_size,1,8});\n",
    "    torch::Tensor prefix = torch::zeros({batch_size,8});\n",
    "    torch::Tensor lengths = torch::zeros({batch_size,1});\n",
    "    torch::Tensor cur_coord = torch::zeros({batch_size,1,2});\n",
    "    torch::Tensor done = torch::zeros({batch_size,1});\n",
    "    torch::Tensor logprob_so_far = torch::zeros({batch_size,1});\n",
    "    torch::Tensor bound_togo = torch::zeros({batch_size,1});\n",
    "    torch::Tensor max_gumbel = torch::zeros({batch_size,1});\n",
    "    torch::Tensor is_t_opt = torch::zeros({batch_size});\n",
    "     \n",
    "    Convertor(py::list )\n",
    "    \n",
    "    torch::Tensor operator[](int i) const\n",
    "};\n",
    "\n",
    "/*\n",
    "py::list stack_heap_nodes_vec(vector<HeapNode> heap_nodes){\n",
    "    int batch_size = heap_nodes.size();\n",
    "    torch::Tensor ids = torch::zeros({batch_size,1});\n",
    "    torch::Tensor t = torch::zeros({batch_size,1});\n",
    "    torch::Tensor next_actions = torch::zeros({batch_size,1,8});\n",
    "    torch::Tensor not_visited = torch::zeros({batch_size,1,8});\n",
    "    torch::Tensor prefix = torch::zeros({batch_size,8});\n",
    "    torch::Tensor lengths = torch::zeros({batch_size,1});\n",
    "    torch::Tensor cur_coord = torch::zeros({batch_size,1,2});\n",
    "    torch::Tensor done = torch::zeros({batch_size,1});\n",
    "    torch::Tensor logprob_so_far = torch::zeros({batch_size,1});\n",
    "    torch::Tensor bound_togo = torch::zeros({batch_size,1});\n",
    "    torch::Tensor max_gumbel = torch::zeros({batch_size,1});\n",
    "    torch::Tensor is_t_opt = torch::zeros({batch_size});\n",
    "\n",
    "    torch::Tensor ids_object;\n",
    "    py::object t_object;\n",
    "    py::object next_actions_object;\n",
    "    py::object not_visited_object;\n",
    "    py::object prefix_object;\n",
    "    py::object lengths_object;\n",
    "    py::object cur_coord_object;\n",
    "    py::object done_object;\n",
    "    py::object logprob_so_far_object;\n",
    "    py::object bound_togo_object;\n",
    "    py::object max_gumbel_object;\n",
    "    py::object is_t_opt_object;\n",
    "    for (int i=0; i < batch_size; i++){\n",
    "        py::list temp_ids = heap_nodes[i].python_node_object.attr(\"ids\");\n",
    "        ids_object = temp_ids[heap_nodes[i].idx_original];\n",
    "        ids[i] = temp_ids[heap_nodes[i].idx_original];\n",
    "\n",
    "        //ids[i] = heap_nodes[i].python_node_object.attr(\"ids\")[heap_nodes[i].idx_original];\n",
    "        t[i]  = heap_nodes[i].python_node_object.t[heap_nodes[i].idx_original];\n",
    "        next_actions[i] = heap_nodes[i].python_node_object.next_actions[heap_nodes[i].idx_original];\n",
    "        not_visited[i] = heap_nodes[i].python_node_object.not_visited[heap_nodes[i].idx_original];\n",
    "        prefix[i] = heap_nodes[i].python_node_object.prefix[heap_nodes[i].idx_original];\n",
    "        lengths[i] = heap_nodes[i].python_node_object.lengths[heap_nodes[i].idx_original];\n",
    "        cur_coord[i] = heap_nodes[i].python_node_object.cur_coord[heap_nodes[i].idx_original];\n",
    "        done[i] = heap_nodes[i].python_node_object.done[heap_nodes[i].idx_original];\n",
    "        logprob_so_far[i] = heap_nodes[i].python_node_object.logprob_so_far[heap_nodes[i].idx_original];\n",
    "        bound_togo[i] = heap_nodes[i].python_node_object.bound_togo[heap_nodes[i].idx_original];\n",
    "        max_gumbel[i] = heap_nodes[i].python_node_object.max_gumbel[heap_nodes[i].idx_original];\n",
    "        is_t_opt[i] = heap_nodes[i].python_node_object.is_t_opt[heap_nodes[i].idx_original];\n",
    "    }\n",
    "\n",
    "    py::list to_return(ids, t,next_actions, not_visited, prefix, lengths, cur_coord, done, logprob_so_far,bound_togo,max_gumbel,is_t_opt);\n",
    "    return to_return;\n",
    "}\n",
    "*/\n",
    "struct Trajectory{\n",
    "    int idx;\n",
    "    py::object actions;  //this is actually a tensor\n",
    "    float cost;\n",
    "    float objective;\n",
    "\n",
    "    Trajectory(HeapNode leaf_node){\n",
    "        py::list batch_objective(leaf_node.python_node_object.attr(\"objective\"));\n",
    "        py::list batch_lengths(leaf_node.python_node_object.attr(\"lengths\"));\n",
    "        py::list batch_actions(leaf_node.python_node_object.attr(\"prefix\"));\n",
    "\n",
    "        idx = leaf_node.idx_original;\n",
    "        objective = batch_objective[idx].cast<float>();\n",
    "        cost = batch_lengths[idx].cast<float>();\n",
    "\n",
    "\n",
    "        actions = batch_actions[idx];\n",
    "    }\n",
    "};\n",
    "\n",
    "class Heap {\n",
    " private:\n",
    "    std::vector<HeapNode> elements_;\n",
    " public:\n",
    "    Heap(HeapNode new_node){\n",
    "        elements_.push_back(new_node);\n",
    "        push_heap(elements_.begin(), elements_.end());\n",
    "    };\n",
    "    void push(HeapNode new_node) {\n",
    "\n",
    "        elements_.push_back(new_node);\n",
    "        push_heap(elements_.begin(), elements_.end());\n",
    "    }\n",
    "\n",
    "    HeapNode pop() {\n",
    "      HeapNode result = elements_.front();\n",
    "      pop_heap(elements_.begin(), elements_.end());\n",
    "      elements_.pop_back();\n",
    "      return result;\n",
    "    }\n",
    "\n",
    "    py::object get_elements_(){\n",
    "         vector<double> r;\n",
    "         vector<bool> b;\n",
    "         for (int i=0; i<elements_.size(); i++){\n",
    "            r.push_back(elements_[i].priority);\n",
    "            b.push_back(elements_[i].t_opt);\n",
    "         }\n",
    "         return py::make_tuple(py::cast(r),py::cast(b));\n",
    "    };\n",
    "    bool is_empty() {\n",
    "        return elements_.empty();\n",
    "    };\n",
    "};\n",
    "\n",
    "class BatchedHeaps{\n",
    "    private:\n",
    "        int batch_size_ = 100;\n",
    "    public:\n",
    "        vector<Heap> heaps_;\n",
    "\n",
    "        BatchedHeaps(py::list root_nodes){\n",
    "            batch_size_ = py::len(root_nodes);\n",
    "            for (int sample_idx=0; sample_idx < batch_size_; sample_idx++){\n",
    "                heaps_.push_back(Heap(HeapNode(root_nodes[sample_idx])));\n",
    "            }\n",
    "        }\n",
    "\n",
    "        py::list pop_batch(){\n",
    "            py::list heap_nodes;\n",
    "            py::list trajectories;\n",
    "            py::list to_return;\n",
    "            for (int sample_idx=0; sample_idx < batch_size_; sample_idx++){\n",
    "                bool to_pop = true; //!heaps_[sample_idx].is_empty();\n",
    "                while (to_pop){\n",
    "                    HeapNode node(heaps_[sample_idx].pop());\n",
    "                    to_pop = node.done;\n",
    "                    if(to_pop){\n",
    "                        Trajectory traj(node);\n",
    "                        trajectories.append(traj);\n",
    "                        }\n",
    "                     else{\n",
    "                        heap_nodes.append(node);\n",
    "                     }\n",
    "                }\n",
    "\n",
    "            }\n",
    "            to_return.append(heap_nodes);\n",
    "            to_return.append(trajectories);\n",
    "            return to_return;\n",
    "        }\n",
    "\n",
    "        /*\n",
    "        py::list pop_batch(){\n",
    "            vector<HeapNode> heap_nodes;\n",
    "            py::list trajectories;\n",
    "            py::list to_return;\n",
    "            for (int sample_idx=0; sample_idx < batch_size_; sample_idx++){\n",
    "                bool to_pop = true; //!heaps_[sample_idx].is_empty();\n",
    "                while (to_pop){\n",
    "                    HeapNode node(heaps_[sample_idx].pop());\n",
    "                    to_pop = node.done;\n",
    "                    if(to_pop){\n",
    "                        Trajectory traj(node);\n",
    "                        trajectories.append(traj);\n",
    "                        }\n",
    "                     else{\n",
    "                        heap_nodes.push_back(node);\n",
    "                     }\n",
    "                }\n",
    "\n",
    "            }\n",
    "            py::list torch_heap_node = stack_heap_nodes_vec(heap_nodes);\n",
    "            to_return.append(torch_heap_node);\n",
    "            to_return.append(trajectories);\n",
    "            return to_return;\n",
    "        }\n",
    "        */\n",
    "        void push_batch(py::list heap_nodes, torch::Tensor ignore_list){\n",
    "            for (int sample_idx=0; sample_idx < batch_size_; sample_idx++){\n",
    "                if (!ignore_list[sample_idx].item<bool>()){\n",
    "                    heaps_[sample_idx].push(HeapNode(heap_nodes[sample_idx]));\n",
    "                    }\n",
    "            }\n",
    "        }\n",
    "        void push_batch(py::list heap_nodes){\n",
    "            for (int sample_idx=0; sample_idx < batch_size_; sample_idx++){\n",
    "                heaps_[sample_idx].push(HeapNode(heap_nodes[sample_idx]));\n",
    "\n",
    "            }\n",
    "        }\n",
    "        int size(){\n",
    "            return heaps_.size();\n",
    "        }\n",
    "};\n",
    "\n",
    "\n",
    "namespace py = pybind11;\n",
    "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
    "\n",
    "  m.def(\"padded_logsumexp\", &padded_logsumexp, \"padded logsumexp\");\n",
    "\n",
    "  py::class_<BatchedHeaps>(m, \"BatchedHeaps\") // returns BatchedHeapNode\n",
    "      .def(py::init<py::list>())\n",
    "      //.def(py::init<py::object>())\n",
    "      .def(\"pop_batch\", &BatchedHeaps::pop_batch)\n",
    "      .def(\"push_batch\",(void (BatchedHeaps::*)(py::list)) &BatchedHeaps::push_batch)\n",
    "      .def(\"push_batch\",(void (BatchedHeaps::*)(py::list, torch::Tensor)) &BatchedHeaps::push_batch)\n",
    "      .def(\"size\", &BatchedHeaps::size)\n",
    "      .def_readonly(\"heaps_\", &BatchedHeaps::heaps_);\n",
    "\n",
    "  py::class_<HeapNode>(m, \"HeapNode\")\n",
    "      //.def(py::init<double,double,bool,double>())\n",
    "      .def(py::init<py::list>())\n",
    "      .def_readwrite(\"python_node_object\", &HeapNode::python_node_object)\n",
    "      .def_readwrite(\"priority\", &HeapNode::priority)\n",
    "      .def_readwrite(\"done\", &HeapNode::done)\n",
    "      .def_readwrite(\"idx_original\", &HeapNode::idx_original)\n",
    "      .def_readwrite(\"t_opt\", &HeapNode::t_opt);\n",
    "\n",
    "\n",
    "  py::class_<Trajectory>(m, \"Trajectory\")\n",
    "      .def(py::init<HeapNode>())\n",
    "      .def_readonly(\"idx\", &Trajectory::idx)\n",
    "      .def_readonly(\"actions\", &Trajectory::actions)\n",
    "      .def_readonly(\"cost\", &Trajectory::cost)\n",
    "      .def_readonly(\"objective\", &Trajectory::objective);\n",
    "\n",
    "  py::class_<Heap>(m, \"Heap\")\n",
    "      .def(py::init<HeapNode>())\n",
    "      .def(\"get_elements_\", &Heap::get_elements_)\n",
    "      .def(\"push\", &Heap::push)\n",
    "      .def(\"pop\", &Heap::pop) //\n",
    "      .def(\"is_empty\", &Heap::is_empty); //pybind11::return_value_policy::move\n",
    "\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
